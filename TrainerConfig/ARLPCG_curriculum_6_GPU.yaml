behaviors:
  Generator:
    trainer_type: ppo
  
    max_steps: 1.5e5
    time_horizon: 64
    summary_freq: 250
    checkpoint_interval: 1000

    hyperparameters:
      batch_size: 128
      beta: 0.0001
      buffer_size: 2048
      epsilon: 0.2
      lambd: 0.95
      learning_rate: 0.003
      learning_rate_schedule: linear
      num_epoch: 3    


    network_settings:
      vis_encode_type: simple
      num_layers: 2
      normalize: false
      hidden_units: 256
      memory: null

    reward_signals:
      extrinsic:
        strength: 1.0
        gamma: 0.99
        
  Solver:
    trainer_type: ppo
  
    max_steps: 1.5e6
    time_horizon: 64
    summary_freq: 1000
    checkpoint_interval: 10000

    hyperparameters:
      batch_size: 128
      beta: 0.001
      buffer_size: 2048
      epsilon: 0.2
      lambd: 0.95
      learning_rate: 0.0008
      learning_rate_schedule: linear
      num_epoch: 3    


    network_settings:
      vis_encode_type: simple
      num_layers: 2
      normalize: false
      hidden_units: 256
      memory: null


    reward_signals:
      extrinsic:
        strength: 1.0
        gamma: 0.99
    threaded: true
engine_settings:
  width: 84
  height: 84
  quality_level: 5
  time_scale: 10
  target_frame_rate: -1
  capture_frame_rate: 60
  
environment_parameters:
# To randomize an environment parameter
#  aux_input:
#    sampler_type: uniform
#    sampler_parameters:
#        min_value: -1.0
#        max_value: -.6
  aux_input:
    curriculum:
      - name: Lesson1 # The '-' is important as this is a list
        completion_criteria:
          measure: reward
          behavior: Solver
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 1
        value:
          sampler_type: multirangeuniform
          sampler_parameters:
            intervals: [[-1.01, -1], [1, 1.01]]

